smartbind(comment)
article_comment_bigram <- article_comment %>%
unnest_tokens(bigram, artContent, token = jieba_bigram)
kable(head(article_comment_bigram, 100)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# load devotion_lexicon
user_dict <- scan(file = "./dict/user_dict.txt", what=character(),sep='\n',
encoding='utf-8',fileEncoding='utf-8')
stop_words_df <- fread(file = "./dict/stop_words.txt", sep='\n'
,encoding='UTF-8', colClasses="character")
stop_words <- stop_words_df %>% pull(1)
negation_words <- scan(file = "./dict/negation_words.txt", what=character(),sep='\n')
# 將bigram拆成word1和word2
bigrams_separated <- article_comment_bigram %>%
filter(!str_detect(bigram, regex("[0-9a-zA-Z]"))) %>%
separate(bigram, c("word1", "word2"), sep = " ")
# 並選出word2爲情緒詞
#去除wrod1與word2都是stop word
bigrams_separated  <- bigrams_separated %>%
filter(!(word1 %in% stop_words & word2 %in% stop_words))
article_comment_sentiment_bigrams <- rbind(
bigrams_separated %>%
filter(source == "nsysu_dcard") %>%
merge(nsysu_ch , by.x='word2', by.y='word')
,
bigrams_separated %>%
filter(source == "nuk_dcard") %>%
merge(nuk_ch , by.x='word2', by.y='word')
,
bigrams_separated %>%
filter(source == "ptt" | source == "exam_dcard") %>%
merge(LIWC_ch , by.x='word2', by.y='word')
)
article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>% select(id,	 artDate,	 source,	 artTitle,	 word1, word2,	 sentiment)
kable(article_comment_sentiment_bigrams) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 計算情緒值，positive為1，negative為-1
article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>% rename(sentiment_tag = sentiment)
article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>%
mutate(sentiment = ifelse(sentiment_tag == "positive",1,-1)) %>%
select(source, artDate, word1, word2, sentiment_tag, sentiment)
kable(article_comment_sentiment_bigrams) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 產生討論併校文章最早到最近的每天的日期
all_dates <-
expand.grid(seq(as.Date(min(article_comment_sentiment_bigrams$artDate)), as.Date(max(article_comment_sentiment_bigrams$artDate)), by="day"), c("positive", "negative"))
names(all_dates) <- c("artDate", "sentiment")
# 計算每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
sentiment_plot_data <- all_dates %>%
merge(sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
# 計算我們資料集中 每日的情緒值
source_sentiment_plot_data <- article_comment_sentiment_bigrams %>%
group_by(source, artDate,sentiment_tag) %>%
summarise(count=n())
#計算各來源情緒
source_sentiment_plot_data <- rbind(
all_dates %>%
merge(source_sentiment_plot_data %>%
filter(source == "nsysu_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nsysu_dcard"),
all_dates %>%
merge(source_sentiment_plot_data %>%
filter(source == "nuk_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nuk_dcard"),
all_dates %>%
merge(source_sentiment_plot_data %>%
filter(source == "exam_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "exam_dcard"),
all_dates %>%
merge(source_sentiment_plot_data %>%
filter(source == "ptt")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "ptt")
) %>%
mutate(count = replace_na(count, 0))
# 畫圖
source_sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d")) +
facet_wrap(~source)
# 查看前面有否定詞的情緒詞
kable(article_comment_sentiment_bigrams %>%
filter(word1 %in% negation_words) %>%
count(word1, word2, sort = TRUE) ) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
kable(article_comment_sentiment_bigrams_negated) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 計算每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
negated_sentiment_plot_data <- all_dates %>%
merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 畫圖
negated_sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
# 合併兩種情緒值的資料
all_sentiments <- bind_rows(
sentiment_plot_data %>% mutate(sentiment=paste(sentiment, "_original", sep = "")),
negated_sentiment_plot_data %>% mutate(sentiment=paste(sentiment, "_negated", sep = "")))
kable(all_sentiments) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 正面情緒
all_sentiments %>%
filter(sentiment %in% c("positive_negated", "positive_original")) %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d"))
# 負面情緒
all_sentiments %>%
filter(sentiment %in% c("negative_original", "negative_negated")) %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d"))
# ngram function, where n=11
ngram_11 <- function(t) {
lapply(t, function(x) {
if(nchar(x)>1){
tokens <- segment(x, jieba_tokenizer)
ngram<- ngrams(tokens, 11)
ngram <- lapply(ngram, paste, collapse = " ")
unlist(ngram)
}
})
}
# 執行ngram_11進行分詞
article_comment_ngram_11 <- article_comment %>%
select(id, artContent) %>%
unnest_tokens(ngram, artContent, token = ngram_11) %>%
filter(!str_detect(ngram, regex("[0-9a-zA-Z]")))
# 將ngram拆成word1 ~ word11
ngrams_11_separated <- article_comment_ngram_11 %>%
separate(ngram, paste0("word", c(1:11),sep=""), sep = " ")
# 尋找 "中山" 出現的前後五個詞彙
tu_five_words <- ngrams_11_separated %>%
filter((word6=="中山"))
kable(tu_five_words) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 尋找 "中山" 的前後5個詞中常出現哪些的詞彙
tu_five_words_count <- tu_five_words %>%
melt(id.vars = "id", measure.vars = paste0("word", c(1:11),sep="")) %>%
rename(word=value) %>%
filter(variable!="word6") %>%
filter(!(word %in% stop_words), nchar(word)>1) %>%
count(word, sort = TRUE)
kable(tu_five_words_count) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 畫圖顯示
tu_five_words_count %>%
arrange(desc(abs(n))) %>%
head(20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = n > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words near by \"中山") +
ylab("Word count") +
coord_flip()+
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
# 計算兩個詞彙同時出現的總次數
id_data_tokens <- data_tokens %>%
count(id, word, sort = TRUE)
word_pairs <- id_data_tokens %>%
pairwise_count(word, id, sort = TRUE)
kable(head(word_pairs, 100)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 計算兩個詞彙間的相關性
word_cors <- id_data_tokens %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, id, sort = TRUE)
kable(head(word_cors,20)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
# 顯示相關性大於0.398的組合
set.seed(2019)
word_cors %>%
filter(correlation > .398) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), repel = TRUE, family = "Heiti TC Light") + #加入中文字型設定，避免中文字顯示錯誤。
theme_void()
#計算各來源的總詞彙數
data_tokens_count <- data_tokens %>%
count(source, word)
total_words_counts <- data_tokens_count %>%
group_by(source) %>%
summarise(total = sum(n))
total_words <- left_join(data_tokens_count, total_words_counts)
#bind tf-idf
total_words_tf_idf <- total_words %>%
bind_tf_idf(word, source, n)
kable(total_words_tf_idf ) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
total_words_tf_idf %>%
group_by(source) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = source)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~source, ncol = 2, scales = "free") +
coord_flip()
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
article_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
article_tokens$word = clean(article_tokens$word)
article_tokens = article_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
filter(!word == "")
article_comment_tokens <- article_tokens %>% smartbind(comment_tokens)
article_comment_tokens <- article_comment_tokens %>%
count(id,artTitle,artDate,source,word) %>%
rename(count=n)
article_comment_tokens
tidy_merge <- article_comment_tokens %>%
select(artTitle,word,count)
tidy_merge
merge_tf_idf <- tidy_merge %>%
bind_tf_idf(word, artTitle, count) %>%
arrange(desc(tf_idf))
merge_tf_idf
term_avg_tfidf=merge_tf_idf %>%
group_by(word) %>%
summarise(
tfidf_avg=mean(tf_idf)
)
term_avg_tfidf<-term_avg_tfidf %>% arrange(desc(tfidf_avg))
term_avg_tfidf$tfidf_avg %>% summary
term_remove=term_avg_tfidf %>%
filter(tfidf_avg<0.007099) %>%
.$word
term_remove %>% head
dtm_merge = merge_tf_idf %>%
filter(!word %in% term_remove) %>%
cast_dtm(document=artTitle,term=word,value= count)
dtm_merge
dtm_matrix=dtm_merge %>% as.data.frame.matrix
dtm_matrix[1:10,1:20]
library(doParallel)
clust = makeCluster(detectCores())
registerDoParallel(clust); getDoParWorkers()
t0 = Sys.time()
d=dtm_matrix %>%
dist(method="euclidean")  #歐式距離，算文章與文章之間的距離
Sys.time() - t0
t0 = Sys.time()
hc = hclust(d, method='ward.D')   # 階層式分群 的 華德法
plot(hc, labels = FALSE)
rect.hclust(hc, k=6, border="red")  #會由上而下，依照距離，分出在計算hclust時最後三個才合併的cluster
Sys.time() - t0
kg = cutree(hc, k=6)
L = split(dtm_matrix, kg)
L$`1`[1:10,1:10]
sapply(L, function(x) x%>% colMeans %>% sort %>% tail %>% names)
t0 = Sys.time()
n = 1000 #n個字
tsne = dtm_merge[, 1:n] %>% as.data.frame.matrix %>%
scale %>% t %>% Rtsne(
check_duplicates = FALSE, theta=0.0, max_iter=3200)
Sys.time()-t0
Y = tsne$Y              # tSNE coordinates
d_Y = dist(Y)             # distance matrix
hc_Y = hclust(d_Y )          # hi-clustering
plot(hc_Y,label=F)
rect.hclust(hc_Y, k=12, border="red")
K = 12              # number of clusters
g = cutree(hc_Y,K)        # cut into K clusters
table(g) %>% as.vector %>% sort         # sizes of clusters
library(randomcoloR)
library(wordcloud)
wc = col_sums(dtm_merge[,1:n]) #n個字
colors = distinctColorPalette(K)
png("./merge.png", width=3200, height=1800)#輸出圖片到路徑下
textplot(
Y[,1], Y[,2], colnames(dtm_merge)[1:n], show=F,
col=colors[g],
cex= 0.3 + 1.25 * sqrt(wc/mean(wc)),
font=2)
dev.off()
merge_dtm <- article_comment_tokens %>% cast_dtm(id, word, count)
merge_dtm
inspect(merge_dtm[1:10,1:10])
merge_lda <- LDA(merge_dtm, k = 4, control = list(seed = 1))
merge_topics <- tidy(merge_lda, matrix = "beta")
# 注意，在tidy function裡面要使用"beta"來取出Phi矩陣。
merge_topics
#從merge_topics中可以得到特定主題生成特定詞彙的概率。
merge_top_terms <- merge_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
merge_top_terms
merge_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
remove_words <- c("中山", "併","高大","合併","學校","大學","中山大學","高雄大學","說","校","人","想","高","好","學生","知道","覺得","高雄","後","喔")
merge_top_terms <- merge_topics %>%
filter(! term %in% remove_words) %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
merge_top_terms %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(magrittr)
library(dplyr)
a <- article_comment %>% select(artContent)
#1
beta_spread_1 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic1 / (topic2+topic3+topic4)))
merge_topic_ratio_1 <- beta_spread_1 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=1)
merge_topic_ratio_1
#2
beta_spread_2 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic2 / (topic1+topic3+topic4)))
merge_topic_ratio_2 <- beta_spread_2 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=2)
merge_topic_ratio_2
#3
beta_spread_3 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic3 / (topic1+topic2+topic4)))
merge_topic_ratio_3 <- beta_spread_3 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=3)
merge_topic_ratio_3
#4
beta_spread_4 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic4 / (topic1+topic2+topic3)))
merge_topic_ratio_4 <- beta_spread_4 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=4)
merge_topic_ratio_4
merge_topic_ratio_all <- rbind(merge_topic_ratio_1,merge_topic_ratio_2,merge_topic_ratio_3,merge_topic_ratio_4)
merge_topic_ratio_all %>%
ggplot(aes(x = reorder(term, log_ratio), y = log_ratio , fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
xlab("word")+
coord_flip()
topicmodels_json_ldavis <- function(fitted, doc_term){
require(LDAvis)
require(slam)
# Find required quantities
phi <- as.matrix(posterior(fitted)$terms)
theta <- as.matrix(posterior(fitted)$topics)
vocab <- colnames(phi)
term_freq <- slam::col_sums(doc_term)
# Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = as.vector(table(doc_term$i)),
term.frequency = term_freq)
return(json_lda)
}
remove_words <- c("中山", "併","高大","合併","學校","大學","中山大學","高雄大學","說","校","人","想","高","好","學生","知道","覺得","高雄","後","喔")
merge_top_terms <- merge_topics %>%
filter(! term %in% remove_words) %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
merge_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(magrittr)
library(dplyr)
a <- article_comment %>% select(artContent)
#1
beta_spread_1 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic1 / (topic2+topic3+topic4)))
merge_topic_ratio_1 <- beta_spread_1 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=1)
merge_topic_ratio_1
#2
beta_spread_2 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic2 / (topic1+topic3+topic4)))
merge_topic_ratio_2 <- beta_spread_2 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=2)
merge_topic_ratio_2
#3
beta_spread_3 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic3 / (topic1+topic2+topic4)))
merge_topic_ratio_3 <- beta_spread_3 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=3)
merge_topic_ratio_3
#4
beta_spread_4 <- merge_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .0004 | topic2 > .0004 | topic3 > .0004 | topic4 > .0004) %>%
mutate(log_ratio = log2(topic4 / (topic1+topic2+topic3)))
merge_topic_ratio_4 <- beta_spread_4 %>% top_n(10,wt = log_ratio) %>%
arrange(desc(log_ratio)) %>% mutate(topic=4)
merge_topic_ratio_4
merge_topic_ratio_all <- rbind(merge_topic_ratio_1,merge_topic_ratio_2,merge_topic_ratio_3,merge_topic_ratio_4)
merge_topic_ratio_all %>%
ggplot(aes(x = reorder(term, log_ratio), y = log_ratio , fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
xlab("word")+
coord_flip()
topicmodels_json_ldavis <- function(fitted, doc_term){
require(LDAvis)
require(slam)
# Find required quantities
phi <- as.matrix(posterior(fitted)$terms)
theta <- as.matrix(posterior(fitted)$topics)
vocab <- colnames(phi)
term_freq <- slam::col_sums(doc_term)
# Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = as.vector(table(doc_term$i)),
term.frequency = term_freq)
return(json_lda)
}
merge_lda <- LDA(merge_dtm, k = 4, control = list(seed = 1))
# 設置alpha及delta參數
#devotion_lda_removed <- LDA(devotion_dtm_removed, k = 4, method = "Gibbs", control = list(seed = 1234, alpha = 2, delta= 0.1))
json_res <- topicmodels_json_ldavis(merge_lda,merge_dtm)
serVis(json_res, out.dir = "vis", open.browser = T)
writeLines(iconv(readLines("./vis/lda.json")),
file("./vis/lda.json", encoding="UTF-8"))
