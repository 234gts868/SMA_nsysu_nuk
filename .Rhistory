mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~word1, ncol = 2, scales = "free") +
coord_flip()
negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>%
filter(word1 %in% negation_words) %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word1, word2, score, sort = TRUE)
negated_words %>%
mutate(contribution = n * score) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~word1, ncol = 3, scales = "free") +
coord_flip()
negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>%
filter(word1 %in% negation_words) %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word1, word2, score, sort = TRUE)
negated_words %>%
mutate(contribution = n * score) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~word1, ncol = 1, scales = "free") +
coord_flip()
negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>%
filter(word1 %in% negation_words) %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word1, word2, score, sort = TRUE)
negated_words %>%
mutate(contribution = n * score) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~word1, ncol = 2, scales = "free") +
coord_flip()
negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>%
filter(word1 %in% negation_words) %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word1, word2, score, sort = TRUE)
negated_words %>%
mutate(contribution = n * score) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~word1, ncol = 2) +
coord_flip()
negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>%
filter(word1 %in% negation_words) %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word1, word2, score, sort = TRUE)
negated_words %>%
mutate(contribution = n * score) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~word1, ncol = 2, scales = "free") +
coord_flip()
bigram_counts
bigram_counts
bigram_graph <- bigram_counts %>%
filter(n > 1) %>%
graph_from_data_frame()
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra', "ggplot2",'readr','data.table','reshape2','wordcloud','tidyr','scales','jiebaR','sentimentr','htmltools','igraph')
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(sentimentr)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(wordcloud)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(igraph)
bigram_counts
bigram_graph <- bigram_counts %>%
filter(n > 1) %>%
graph_from_data_frame()
bigram_graph
bigram_counts
bigram_graph <- bigram_counts %>%
filter(n > 1) %>%
graph_from_data_frame()
bigram_graph
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
library(dplyr)
library(dplyr)
library(dplyr)
library(janeaustenr)
library(tidytext)
library(ggplot2)
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE)
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
ggplot(book_words, aes(n/total, fill = book)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
freq_by_rank <- book_words %>%
group_by(book) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = book)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
rank_subset <- freq_by_rank %>%
filter(rank < 500,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = book)) +
geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
book_words <- book_words %>%
bind_tf_idf(word, book, n)
book_words
book_words %>%
select(-total) %>%
arrange(desc(tf_idf))
book_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(book) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(word, tf_idf, fill = book)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~book, ncol = 2, scales = "free") +
coord_flip()
library(gutenbergr)
physics <- gutenberg_download(c(37729, 14725, 13476, 5001),
meta_fields = "author")
physics_words <- physics %>%
unnest_tokens(word, text) %>%
count(author, word, sort = TRUE)
physics_words
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
library(stringr)
physics %>%
filter(str_detect(text, "eq\\.")) %>%
select(text)
physics %>%
filter(str_detect(text, "K1")) %>%
select(text)
physics %>%
filter(str_detect(text, "AK")) %>%
select(text)
mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn",
"fig", "file", "cg", "cb", "cm"))
physics_words <- anti_join(physics_words, mystopwords, by = "word")
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
ggplot(plot_physics, aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
class(physics_words$word)
level(physics_words$word)
levels(physics_words$word)
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
factor(physics_words$word)
physics_words %>%
mutate(word = factor(word, levels = rev(unique(word))))
levels(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word))))) %>% select(word)
levels(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factorphysics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factors(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
test <- factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
test <- physics_words %>%
mutate(word = factor(word, levels = rev(unique(word))))
factor(test$word)
Sys.setlocale("LC_CTYPE", "cht")
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra','NLP', "ggplot2",'readr','data.table','reshape2','tidyr','scales','jiebaR','htmltools',"ggraph", "igraph", "reshape2", "widyr","gtools")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(NLP)
library(ggraph)
library(igraph)
library(reshape2)
library(widyr)
library(gtools)
library(htmltools)
unlink('D:/mid/SMA_nsysu_nuk/midtermproject_cache', recursive = TRUE)
getwd()
setwd("D:/mid/midternproject/")
setwd("D:\\mid\\midternproject")
setwd("D:\\mid\\SMA_nsysu_nuk")
data_tokens_count %>% wordcloud2()
Sys.setlocale("LC_CTYPE", "cht")
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra','NLP', "ggplot2",'readr','data.table','reshape2','tidyr','scales','jiebaR','htmltools',"ggraph", "igraph", "reshape2", "widyr","gtools")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(NLP)
library(ggraph)
library(igraph)
library(reshape2)
library(widyr)
library(gtools)
library(htmltools)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
#過濾有出現關於併校資訊關鍵字的文章，並且將增加來源的欄位
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
filter(!(grepl("愛滋",content)|grepl("愛滋",title))) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
filter(!(grepl("山手線|賽馬|手線|高雄縣市合併|又老又窮|清大或交大合校",artTitle)|grepl("山手線|賽馬|手線|大甲",artContent))) %>%
mutate(source = "ptt")
#將ptt的資料_id 改成統一的id
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
#透過rbind將文章資料合起來
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
kable(article) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
nsysu_comment = nsysu_comment %>%
filter(nchar(nsysu_comment$content)>5)%>%
mutate(source="nsysu_dcard")
nuk_comment = nuk_comment %>%
filter(nchar(nuk_comment$content)>5)%>%
mutate(source="nuk_dcard")
exam_comment = exam_comment %>%
filter(nchar(exam_comment$content)>5)%>%
mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
filter(nchar(ptt_comment$content)>5)%>%
mutate(source="ptt")
comment = nsysu_comment %>%
select(id,Date ,source ,source,content)%>%
rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
rbind(exam_comment %>% select(id,Date ,source ,content))
comment =comment %>%
rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment <- comment %>%
filter(id %in% article$id)
kable(comment) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
data = article%>%
smartbind(comment)
kable(data) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
data_count_by_date <- data %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
kable(head(data_count_by_date, 20)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
plot_date <-
data_count_by_date %>%
ggplot(aes(x = as.Date(artDate), y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
data_tokens <- data %>%
unnest_tokens(word, artContent, token=tokenizer)
data_tokens$word = clean(data_tokens$word)
data_tokens = data_tokens %>%
filter(!word == "")
data_tokens_count <- data_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
kable(head(data_tokens_count,20)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
data_tokens_count %>% wordcloud2()
data_tokens_count %>% wordcloud2()
#data_tokens_count %>% wordcloud2()
