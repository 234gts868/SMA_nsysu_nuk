<<<<<<< HEAD
# 印出最常見的20個詞彙
head(tokens_count, 20)
tokens_count %>% wordcloud2()
library(data.table)
library(dplyr)
require(tidytext)
require(jiebaR)
library(stringr)
library(wordcloud2)
library(ggplot2)
library(tidyr)
library(scales)
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra', "ggplot2",'readr','data.table','reshape2','wordcloud','tidyr','scales','jiebaR','sentimentr','htmltools')
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(sentimentr)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(wordcloud)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
nsysu_data = fread("my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("comment_exam.csv",encoding = 'UTF-8') #1024
# apple_data =fread("merge_apple_articleMetaData.csv",encoding = 'UTF-8') #蘋果資料不佳
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
mutate(source = "ptt")
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
head(nuk_data,10)
head(nuk_data,10)
head(exam_data,10)
head(ptt_data,10)
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
article
nsysu_comment = nsysu_comment %>%
filter(nchar(nsysu_comment$content)>15)%>% #412
mutate(source="nsysu_dcard")
nuk_comment = nuk_comment %>%
filter(nchar(nuk_comment$content)>15)%>% #501
mutate(source="nuk_dcard")
exam_comment = exam_comment %>%
filter(nchar(exam_comment$content)>15)%>% #978
mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
filter(nchar(ptt_comment$content)>15)%>% #386
mutate(source="ptt")
head(nsysu_comment,10)
head(nuk_comment,10)
head(exam_comment,10)
head(ptt_comment,10)
comment = nsysu_comment %>%
select(id,Date ,source ,source,content)%>%
rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
rbind(exam_comment %>% select(id,Date ,source ,content))
comment =comment %>%
rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment
data_article <- article %>%
select(artDate, id,source) %>%
distinct()
data_comment <- comment %>%
select(artDate, id,source) %>%
distinct()
article_count_by_date <- data_article %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
comment_count_by_date <- data_comment %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
head(article_count_by_date, 20)
head(comment_count_by_date, 20)
plot_date <-
article_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
plot_date <-
comment_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
=======
student_dcard = student_dcard%>%
filter(source %in% c("nsysu_dcard","nuk_dcard"))
student_dcard%>%
group_by(source) %>%
summarise(sum = n()) %>%
ggplot(aes(source,
sum))+
geom_bar(stat="identity", width=0.5,fill="steelblue")+
geom_text(aes(label=sum), vjust=-0.3, size=3.5)
frequency <- student_dcard%>%
dplyr::count(source, word)%>%
group_by(source) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(source, proportion) %>%
gather(source, proportion, `nuk_dcard`)
frequency
ggplot(frequency, aes(x = proportion, y = `nsysu_dcard`, color = abs(`nsysu_dcard` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.2, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75")+
theme(legend.position="none") +
labs(y = "nsysu_dcard", x = "nuk_dcard")
cor.test(data = frequency[frequency$source == "nuk_dcard",],
~ proportion + `nsysu_dcard`)
p <- read_file("dict/positive.txt")
n <- read_file("dict/negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
LIWC_ch <- rbind(positive, negative)
LIWC_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(LIWC_ch)
student_dcard %>%
inner_join(LIWC_ch)
student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum)) %>%
filter(source =="nsysu_dcard")
student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum)) %>%
filter(source =="nuk_dcard")
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
# interaction(source, sentiment)
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)+
facet_grid(~source)
p <- read_file("dict/nsysu_positive.txt")
n <- read_file("dict/nsysu_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nsysu_ch <- rbind(positive, negative)
nsysu_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nsysu_ch)
student_dcard %>%
inner_join(nsysu_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
p <- read_file("dict/nuk_positive.txt")
n <- read_file("dict/nuk_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nuk_ch <- rbind(positive, negative)
nuk_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nuk_ch)
student_dcard %>%
inner_join(nuk_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
#合併文章與留言的tokens
total_tokens <- article_tokens %>%
smartbind(comment_tokens)
#計算各板的總詞彙數
total_tokens_count <- total_tokens %>%
count(source, word)
total_words_counts <- total_tokens_count %>%
group_by(source) %>%
summarise(total = sum(n))
total_words <- left_join(total_tokens_count, total_words_counts)
#bind tf-idf
total_words_tf_idf <- total_words %>%
bind_tf_idf(word, source, n)
total_words_tf_idf
total_words_tf_idf %>%
group_by(source) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = source)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~source, ncol = 2, scales = "free") +
coord_flip()
ggplot(total_words, aes(n/total, fill = source)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.01) +
facet_wrap(~source, ncol = 2, scales = "free_y")
freq_by_rank <- total_words %>%
group_by(source) %>%
arrange(desc(n)) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = source)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
rank_subset <- freq_by_rank %>%
filter(rank < 500,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = source)) +
geom_abline(intercept = -1.417  , slope = -0.730, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
# remove stopwords
jieba_tokenizer = worker()
# unnest_tokens 使用的bigram分詞函數
# Input: a character vector
# Output: a list of character vectors of the same length
jieba_bigram <- function(t) {
>>>>>>> cc5fbef2d7043d336979a87db40fea3c83430fda
lapply(t, function(x) {
if(nchar(x)>1){
tokens <- segment(x, jieba_tokenizer)
bigram<- ngrams(tokens, 2)
bigram <- lapply(bigram, paste, collapse = " ")
unlist(bigram)
}
})
}
article_comment <- article %>%
smartbind(comment)
article_comment_bigram <- article_comment %>%
unnest_tokens(bigram, artContent, token = jieba_bigram)
article_comment_bigram
# load devotion_lexicon
user_dict <- scan(file = "./dict/user_dict.txt", what=character(),sep='\n',
encoding='utf-8',fileEncoding='utf-8')
stop_words_df <- fread(file = "./dict/stop_words.txt", sep='\n'
,encoding='UTF-8', colClasses="character")
stop_words <- stop_words_df %>% pull(1)
negation_words <- scan(file = "./dict/negation_words.txt", what=character(),sep='\n')
# 將bigram拆成word1和word2
# 將包含英文字母或和數字的詞彙清除
bigrams_separated <- article_comment_bigram %>%
filter(!str_detect(bigram, regex("[0-9a-zA-Z]"))) %>%
separate(bigram, c("word1", "word2"), sep = " ")
# 並選出word2爲情緒詞的bigram
#去除wrod1與word2都是stop word
article_comment_sentiment_bigrams <- bigrams_separated %>%
filter(!(word1 %in% stop_words & word2 %in% stop_words)) %>%
filter(word2 %in% LIWC_ch$word)
article_comment_sentiment_bigrams
# 選出word2中，有出現在情緒詞典中的詞彙
# 如果是正面詞彙則賦予： 情緒標籤爲"positive"、情緒值爲  1
# 如果是負面詞彙則賦予： 情緒標籤爲"negative"、情緒值爲 -1
#將正負面詞分開
LIWC_ch_pos <- LIWC_ch %>% filter(sentiment == "positive")
LIWC_ch_neg <- LIWC_ch %>% filter(sentiment == "negative")
article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>%
select(source, artDate, word1, word2) %>%
mutate(sentiment = ifelse(word2 %in% LIWC_ch_pos$word,1,-1), sentiment_tag = ifelse(word2 %in% LIWC_ch_pos$word, "positive", "negative"))
article_comment_sentiment_bigrams
# 生成一個時間段中的 日期和情緒標籤的所有可能組合
all_dates <-
expand.grid(seq(as.Date(min(article_comment_sentiment_bigrams$artDate)), as.Date(max(article_comment_sentiment_bigrams$artDate)), by="day"), c("positive", "negative"))
names(all_dates) <- c("artDate", "sentiment")
all_dates
# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- all_dates %>%
merge(sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
group_by(source, artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- rbind(
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "nsysu_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nsysu_dcard"),
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "nuk_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nuk_dcard"),
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "exam_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "exam_dcard"),
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "ptt")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "ptt")
) %>%
mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d")) +
facet_wrap(~source)
# 查看 前面出現否定詞 且 後面爲情緒詞彙 的組合
article_comment_sentiment_bigrams %>%
filter(word1 %in% negation_words) %>%
count(word1, word2, sort = TRUE)
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- all_dates %>%
merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 最後把圖畫出來
negated_sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(source, artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- rbind(
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "nsysu_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nsysu_dcard"),
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "nuk_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nuk_dcard"),
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "exam_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "exam_dcard"),
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "ptt")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "ptt")
) %>%
mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d")) +
facet_wrap(~source)
# ngram function, where n=11
ngram_11 <- function(t) {
lapply(t, function(x) {
if(nchar(x)>1){
tokens <- segment(x, jieba_tokenizer)
ngram<- ngrams(tokens, 11)
ngram <- lapply(ngram, paste, collapse = " ")
unlist(ngram)
}
})
}
# 執行ngram_11進行分詞
article_comment_ngram_11 <- article_comment %>%
select(id, artContent) %>%
unnest_tokens(ngram, artContent, token = ngram_11) %>%
filter(!str_detect(ngram, regex("[0-9a-zA-Z]")))
article_comment_ngram_11
# 將ngram拆成word1 ~ word11
ngrams_11_separated <- article_comment_ngram_11 %>%
separate(ngram, paste0("word", c(1:11),sep=""), sep = " ")
ngrams_11_separated
# 尋找 "中山" 出現的前後五個詞彙
tu_five_words <- ngrams_11_separated %>%
filter((word6=="中山"))
tu_five_words
# 尋找 "中山" 的前後5個詞中常出現哪些的詞彙
tu_five_words_count <- tu_five_words %>%
melt(id.vars = "id", measure.vars = paste0("word", c(1:11),sep="")) %>%
rename(word=value) %>%
filter(variable!="word6") %>%
filter(!(word %in% stop_words), nchar(word)>1) %>%
count(word, sort = TRUE)
tu_five_words_count
# 畫圖顯示
tu_five_words_count %>%
arrange(desc(abs(n))) %>%
head(20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = n > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words near by \"中山") +
ylab("Word count") +
coord_flip()+
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
# 計算兩個詞彙同時出現的總次數
id_total_tokens <- total_tokens %>%
count(id, word, sort = TRUE)
word_pairs <- id_total_tokens %>%
pairwise_count(word, id, sort = TRUE)
word_pairs
# 計算兩個詞彙間的相關性
word_cors <- id_total_tokens %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, id, sort = TRUE)
word_cors
# 顯示相關性大於0.4的組合
set.seed(2019)
word_cors %>%
filter(correlation > .4) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), repel = TRUE, family = "Heiti TC Light") + #加入中文字型設定，避免中文字顯示錯誤。
theme_void()
View(article_comment)
View(article_comment_sentiment_bigrams)
View(word_cors)
View(word_pairs)
View(article_comment_ngram_11)
View(article_comment_sentiment_bigrams)
View(article_comment_sentiment_bigrams_negated)
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
# 查看 前面出現否定詞 且 後面爲情緒詞彙 的組合
article_comment_sentiment_bigrams %>%
filter(word1 %in% negation_words) %>%
count(word1, word2, sort = TRUE)
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- all_dates %>%
merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 最後把圖畫出來
negated_sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
View(article_comment_sentiment_bigrams_negated)
negative <- strsplit(n, "[,]")[[1]]
p <- read_file("dict/nsysu_positive.txt")
n <- read_file("dict/nsysu_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nsysu_ch <- rbind(positive, negative)
nsysu_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nsysu_ch)
student_dcard %>%
inner_join(nsysu_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nuk_ch)
student_dcard %>%
inner_join(nuk_ch)
p <- read_file("dict/nuk_positive.txt")
n <- read_file("dict/nuk_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nuk_ch <- rbind(positive, negative)
nuk_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
<<<<<<< HEAD
arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)
article_tokens_count %>% wordcloud2()
comment_tokens_count %>% wordcloud2()
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra', "ggplot2",'readr','data.table','reshape2','wordcloud','tidyr','scales','jiebaR','sentimentr','htmltools')
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(sentimentr)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(wordcloud)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
# apple_data =fread("data/merge_apple_articleMetaData.csv",encoding = 'UTF-8') #蘋果資料不佳
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
mutate(source = "ptt")
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
head(nuk_data,10)
head(nuk_data,10)
head(exam_data,10)
head(ptt_data,10)
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
article
nsysu_comment = nsysu_comment %>%
filter(nchar(nsysu_comment$content)>5)%>% #412
mutate(source="nsysu_dcard")
nuk_comment = nuk_comment %>%
filter(nchar(nuk_comment$content)>5)%>% #501
mutate(source="nuk_dcard")
exam_comment = exam_comment %>%
filter(nchar(exam_comment$content)>5)%>% #978
mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
filter(nchar(ptt_comment$content)>5)%>% #386
mutate(source="ptt")
head(nsysu_comment,10)
head(nuk_comment,10)
head(exam_comment,10)
head(ptt_comment,10)
comment = nsysu_comment %>%
select(id,Date ,source ,source,content)%>%
rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
rbind(exam_comment %>% select(id,Date ,source ,content))
comment =comment %>%
rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment
data_article <- article %>%
select(artDate, id,source) %>%
distinct()
data_comment <- comment %>%
select(artDate, id,source) %>%
distinct()
article_count_by_date <- data_article %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
comment_count_by_date <- data_comment %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
head(article_count_by_date, 20)
head(comment_count_by_date, 20)
plot_date <-
article_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
plot_date <-
comment_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論留言數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
artilce_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
artilce_tokens$word = clean(artilce_tokens$word)
artilce_tokens = artilce_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
filter(!word == "")
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- artilce_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
comment_tokens_count <- comment_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)
article_tokens_count %>% wordcloud2()
comment_tokens_count %>% wordcloud2()
article_tokens_count %>%
filter(!word %in% c("中山","大學","中山大學","高大","學校","高雄大學","國立","合校"))%>%
mutate(word = reorder(word, sum)) %>%
top_n(25,sum) %>%
ggplot(aes(word, sum)) +
geom_col() +
xlab(NULL) +
coord_flip()
=======
arrange(desc(sum))%>%
inner_join(nuk_ch)
student_dcard %>%
inner_join(nuk_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
>>>>>>> cc5fbef2d7043d336979a87db40fea3c83430fda
