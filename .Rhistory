mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
library(stringr)
physics %>%
filter(str_detect(text, "eq\\.")) %>%
select(text)
physics %>%
filter(str_detect(text, "K1")) %>%
select(text)
physics %>%
filter(str_detect(text, "AK")) %>%
select(text)
mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn",
"fig", "file", "cg", "cb", "cm"))
physics_words <- anti_join(physics_words, mystopwords, by = "word")
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
ggplot(plot_physics, aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
class(physics_words$word)
level(physics_words$word)
levels(physics_words$word)
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
plot_physics <- physics_words %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf)) %>%
mutate(author = factor(author, levels = c("Galilei, Galileo",
"Huygens, Christiaan",
"Tesla, Nikola",
"Einstein, Albert")))
plot_physics %>%
group_by(author) %>%
top_n(15, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = author)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~author, ncol = 2, scales = "free") +
coord_flip()
factor(physics_words$word)
physics_words %>%
mutate(word = factor(word, levels = rev(unique(word))))
levels(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word))))) %>% select(word)
levels(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factorphysics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factors(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
test <- factor(physics_words %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>% select(word))
test <- physics_words %>%
mutate(word = factor(word, levels = rev(unique(word))))
factor(test$word)
Sys.setlocale("LC_CTYPE", "cht")
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra','NLP', "ggplot2",'readr','data.table','reshape2','tidyr','scales','jiebaR','htmltools',"ggraph", "igraph", "reshape2", "widyr","gtools")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(NLP)
library(ggraph)
library(igraph)
library(reshape2)
library(widyr)
library(gtools)
library(htmltools)
unlink('D:/mid/SMA_nsysu_nuk/midtermproject_cache', recursive = TRUE)
getwd()
setwd("D:/mid/midternproject/")
setwd("D:\\mid\\midternproject")
setwd("D:\\mid\\SMA_nsysu_nuk")
data_tokens_count %>% wordcloud2()
Sys.setlocale("LC_CTYPE", "cht")
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra','NLP', "ggplot2",'readr','data.table','reshape2','tidyr','scales','jiebaR','htmltools',"ggraph", "igraph", "reshape2", "widyr","gtools")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(NLP)
library(ggraph)
library(igraph)
library(reshape2)
library(widyr)
library(gtools)
library(htmltools)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
#過濾有出現關於併校資訊關鍵字的文章，並且將增加來源的欄位
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
filter(!(grepl("愛滋",content)|grepl("愛滋",title))) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
filter(!(grepl("山手線|賽馬|手線|高雄縣市合併|又老又窮|清大或交大合校",artTitle)|grepl("山手線|賽馬|手線|大甲",artContent))) %>%
mutate(source = "ptt")
#將ptt的資料_id 改成統一的id
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
#透過rbind將文章資料合起來
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
kable(article) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
nsysu_comment = nsysu_comment %>%
filter(nchar(nsysu_comment$content)>5)%>%
mutate(source="nsysu_dcard")
nuk_comment = nuk_comment %>%
filter(nchar(nuk_comment$content)>5)%>%
mutate(source="nuk_dcard")
exam_comment = exam_comment %>%
filter(nchar(exam_comment$content)>5)%>%
mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
filter(nchar(ptt_comment$content)>5)%>%
mutate(source="ptt")
comment = nsysu_comment %>%
select(id,Date ,source ,source,content)%>%
rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
rbind(exam_comment %>% select(id,Date ,source ,content))
comment =comment %>%
rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment <- comment %>%
filter(id %in% article$id)
kable(comment) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
data = article%>%
smartbind(comment)
kable(data) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
data_count_by_date <- data %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
kable(head(data_count_by_date, 20)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
plot_date <-
data_count_by_date %>%
ggplot(aes(x = as.Date(artDate), y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
data_tokens <- data %>%
unnest_tokens(word, artContent, token=tokenizer)
data_tokens$word = clean(data_tokens$word)
data_tokens = data_tokens %>%
filter(!word == "")
data_tokens_count <- data_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
kable(head(data_tokens_count,20)) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(height = "300px")
data_tokens_count %>% wordcloud2()
data_tokens_count %>% wordcloud2()
#data_tokens_count %>% wordcloud2()
install.packages("tidytext")
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
library(data.table)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
library(dplyr)
library(stringr)
require(tidytext)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(NLP)
library(ggraph)
library(igraph)
library(reshape2)
library(widyr)
library(gtools)
library(htmltools)
library(data.table)
require(tm)
require(quanteda)
require(Matrix)
require(slam)
require(Rtsne)
require(randomcoloR)
require(magrittr)
require(topicmodels)
require(LDAvis)
require(webshot)
require(htmlwidgets)
require(servr)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
filter(!(grepl("愛滋",content)|grepl("愛滋",title))) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
filter(!(grepl("山手線|賽馬|手線|高雄縣市合併|又老又窮|清大或交大合校",artTitle)|grepl("山手線|賽馬|手線|大甲",artContent))) %>%
mutate(source = "ptt")
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
article
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
article_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
article_tokens$word = clean(article_tokens$word)
article_tokens = article_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
filter(!word == "")
unlink('mid-termCh5Ch6_cache', recursive = TRUE)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
filter(!(grepl("愛滋",content)|grepl("愛滋",title))) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
filter(!(grepl("山手線|賽馬|手線|高雄縣市合併|又老又窮|清大或交大合校",artTitle)|grepl("山手線|賽馬|手線|大甲",artContent))) %>%
mutate(source = "ptt")
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
article
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
article_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
article_tokens$word = clean(article_tokens$word)
article_tokens = article_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens <- comment %>%
tidytext::unnest_tokens(word, artContent, token=tokenizer)
comment_tokens <- comment %>%
jirbaR::unnest_tokens(word, artContent, token=tokenizer)
comment_tokens <- comment %>%
jiebaR::unnest_tokens(word, artContent, token=tokenizer)
comment_tokens <- comment %>%
jiebar::unnest_tokens(word, artContent, token=tokenizer)
t0 = Sys.time()
hc = hclust(d, method='ward.D')   # 階層式分群 的 華德法
