# 印出最常見的20個詞彙
head(tokens_count, 20)
tokens_count %>% wordcloud2()
library(data.table)
library(dplyr)
require(tidytext)
require(jiebaR)
library(stringr)
library(wordcloud2)
library(ggplot2)
library(tidyr)
library(scales)
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra', "ggplot2",'readr','data.table','reshape2','wordcloud','tidyr','scales','jiebaR','sentimentr','htmltools')
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(sentimentr)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(wordcloud)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
nsysu_data = fread("my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("comment_exam.csv",encoding = 'UTF-8') #1024
# apple_data =fread("merge_apple_articleMetaData.csv",encoding = 'UTF-8') #蘋果資料不佳
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
mutate(source = "ptt")
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
head(nuk_data,10)
head(nuk_data,10)
head(exam_data,10)
head(ptt_data,10)
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
article
nsysu_comment = nsysu_comment %>%
filter(nchar(nsysu_comment$content)>15)%>% #412
mutate(source="nsysu_dcard")
nuk_comment = nuk_comment %>%
filter(nchar(nuk_comment$content)>15)%>% #501
mutate(source="nuk_dcard")
exam_comment = exam_comment %>%
filter(nchar(exam_comment$content)>15)%>% #978
mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
filter(nchar(ptt_comment$content)>15)%>% #386
mutate(source="ptt")
head(nsysu_comment,10)
head(nuk_comment,10)
head(exam_comment,10)
head(ptt_comment,10)
comment = nsysu_comment %>%
select(id,Date ,source ,source,content)%>%
rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
rbind(exam_comment %>% select(id,Date ,source ,content))
comment =comment %>%
rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment
data_article <- article %>%
select(artDate, id,source) %>%
distinct()
data_comment <- comment %>%
select(artDate, id,source) %>%
distinct()
article_count_by_date <- data_article %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
comment_count_by_date <- data_comment %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
head(article_count_by_date, 20)
head(comment_count_by_date, 20)
plot_date <-
article_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
plot_date <-
comment_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)%>%
clean(word)
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
clean = function(.,txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)%>%
clean(word)
tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)%>%
clean(word) %>%
filter(!word=="")
View(tokens)
artilce_tokens$word = clean(artilce_tokens$word)
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
artilce_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
artilce_tokens$word = clean(artilce_tokens$word)
artilce_tokens = artilce_tokens %>%
filter(!word == "")
comment_tokens <- comment_tokens %>%
unnest_tokens(word, artContent, token=tokenizer)
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
artilce_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
artilce_tokens$word = clean(artilce_tokens$word)
artilce_tokens = artilce_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
filter(!word == "")
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(tokens_count, 20)
tokens_count %>% wordcloud2()
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
comment_tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(tokens_count, 20)
tokens_count %>% wordcloud2()
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
comment_tokens_count <- tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)
article_tokens_count %>% wordcloud2()
comment_tokens_count %>% wordcloud2()
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- artilce_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
comment_tokens_count <- comment_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)
article_tokens_count %>% wordcloud2()
comment_tokens_count %>% wordcloud2()
jieba_tokenizer <- worker(user="user_dict.txt", stop_word = "stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
artilce_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
artilce_tokens$word = clean(artilce_tokens$word)
artilce_tokens = artilce_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
filter(!word == "")
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- artilce_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
comment_tokens_count <- comment_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)
article_tokens_count %>% wordcloud2()
comment_tokens_count %>% wordcloud2()
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra', "ggplot2",'readr','data.table','reshape2','wordcloud','tidyr','scales','jiebaR','sentimentr','htmltools')
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(dplyr)
library(stringr)
require(tidytext)
library(sentimentr)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(wordcloud)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
# apple_data =fread("data/merge_apple_articleMetaData.csv",encoding = 'UTF-8') #蘋果資料不佳
nsysu_data = nsysu_data %>%
filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
mutate(source = "nsysu_dcard")
nuk_data = nuk_data %>%
filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
mutate(source = "nuk_dcard")
exam_data = exam_data %>%
filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
mutate(source = "exam_dcard")
ptt_data = ptt_data %>%
filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
mutate(source = "ptt")
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"
head(nuk_data,10)
head(nuk_data,10)
head(exam_data,10)
head(ptt_data,10)
article = nsysu_data %>%
select(id,createdAt ,source ,title,content)%>%
rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))
article$artDate = as.Date(article$artDate)
article
nsysu_comment = nsysu_comment %>%
filter(nchar(nsysu_comment$content)>5)%>% #412
mutate(source="nsysu_dcard")
nuk_comment = nuk_comment %>%
filter(nchar(nuk_comment$content)>5)%>% #501
mutate(source="nuk_dcard")
exam_comment = exam_comment %>%
filter(nchar(exam_comment$content)>5)%>% #978
mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
filter(nchar(ptt_comment$content)>5)%>% #386
mutate(source="ptt")
head(nsysu_comment,10)
head(nuk_comment,10)
head(exam_comment,10)
head(ptt_comment,10)
comment = nsysu_comment %>%
select(id,Date ,source ,source,content)%>%
rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
rbind(exam_comment %>% select(id,Date ,source ,content))
comment =comment %>%
rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment
data_article <- article %>%
select(artDate, id,source) %>%
distinct()
data_comment <- comment %>%
select(artDate, id,source) %>%
distinct()
article_count_by_date <- data_article %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
comment_count_by_date <- data_comment %>%
group_by(artDate,source) %>%
summarise(count = n()) %>%
arrange(desc(count))
head(article_count_by_date, 20)
head(comment_count_by_date, 20)
plot_date <-
article_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論文章數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
plot_date <-
comment_count_by_date %>%
ggplot(aes(x = artDate, y = count,colour=source)) +
geom_line(size = 0.5) +
# geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') +
scale_x_date(labels = date_format("%Y/%m/%d" )) +
ggtitle("高大與中山併校 討論留言數") +
xlab("日期") +
ylab("數量") +
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
plot_date
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格
txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
txt }
tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
artilce_tokens <- article %>%
unnest_tokens(word, artContent, token=tokenizer)
artilce_tokens$word = clean(artilce_tokens$word)
artilce_tokens = artilce_tokens %>%
filter(!word == "")
comment_tokens <- comment %>%
unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
filter(!word == "")
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- artilce_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
comment_tokens_count <- comment_tokens %>%
filter(nchar(.$word)>1) %>%
group_by(word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)
article_tokens_count %>% wordcloud2()
comment_tokens_count %>% wordcloud2()
article_tokens_count %>%
filter(!word %in% c("中山","大學","中山大學","高大","學校","高雄大學","國立","合校"))%>%
mutate(word = reorder(word, sum)) %>%
top_n(25,sum) %>%
ggplot(aes(word, sum)) +
geom_col() +
xlab(NULL) +
coord_flip()
