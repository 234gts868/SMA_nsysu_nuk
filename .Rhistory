student_dcard = student_dcard%>%
filter(source %in% c("nsysu_dcard","nuk_dcard"))
student_dcard%>%
group_by(source) %>%
summarise(sum = n()) %>%
ggplot(aes(source,
sum))+
geom_bar(stat="identity", width=0.5,fill="steelblue")+
geom_text(aes(label=sum), vjust=-0.3, size=3.5)
frequency <- student_dcard%>%
dplyr::count(source, word)%>%
group_by(source) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(source, proportion) %>%
gather(source, proportion, `nuk_dcard`)
frequency
ggplot(frequency, aes(x = proportion, y = `nsysu_dcard`, color = abs(`nsysu_dcard` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.2, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75")+
theme(legend.position="none") +
labs(y = "nsysu_dcard", x = "nuk_dcard")
cor.test(data = frequency[frequency$source == "nuk_dcard",],
~ proportion + `nsysu_dcard`)
p <- read_file("dict/positive.txt")
n <- read_file("dict/negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
LIWC_ch <- rbind(positive, negative)
LIWC_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(LIWC_ch)
student_dcard %>%
inner_join(LIWC_ch)
student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum)) %>%
filter(source =="nsysu_dcard")
student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum)) %>%
filter(source =="nuk_dcard")
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
# interaction(source, sentiment)
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)+
facet_grid(~source)
p <- read_file("dict/nsysu_positive.txt")
n <- read_file("dict/nsysu_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nsysu_ch <- rbind(positive, negative)
nsysu_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nsysu_ch)
student_dcard %>%
inner_join(nsysu_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
p <- read_file("dict/nuk_positive.txt")
n <- read_file("dict/nuk_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nuk_ch <- rbind(positive, negative)
nuk_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nuk_ch)
student_dcard %>%
inner_join(nuk_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
#合併文章與留言的tokens
total_tokens <- article_tokens %>%
smartbind(comment_tokens)
#計算各板的總詞彙數
total_tokens_count <- total_tokens %>%
count(source, word)
total_words_counts <- total_tokens_count %>%
group_by(source) %>%
summarise(total = sum(n))
total_words <- left_join(total_tokens_count, total_words_counts)
#bind tf-idf
total_words_tf_idf <- total_words %>%
bind_tf_idf(word, source, n)
total_words_tf_idf
total_words_tf_idf %>%
group_by(source) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = source)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~source, ncol = 2, scales = "free") +
coord_flip()
ggplot(total_words, aes(n/total, fill = source)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.01) +
facet_wrap(~source, ncol = 2, scales = "free_y")
freq_by_rank <- total_words %>%
group_by(source) %>%
arrange(desc(n)) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = source)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
rank_subset <- freq_by_rank %>%
filter(rank < 500,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = source)) +
geom_abline(intercept = -1.417  , slope = -0.730, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
# remove stopwords
jieba_tokenizer = worker()
# unnest_tokens 使用的bigram分詞函數
# Input: a character vector
# Output: a list of character vectors of the same length
jieba_bigram <- function(t) {
lapply(t, function(x) {
if(nchar(x)>1){
tokens <- segment(x, jieba_tokenizer)
bigram<- ngrams(tokens, 2)
bigram <- lapply(bigram, paste, collapse = " ")
unlist(bigram)
}
})
}
article_comment <- article %>%
smartbind(comment)
article_comment_bigram <- article_comment %>%
unnest_tokens(bigram, artContent, token = jieba_bigram)
article_comment_bigram
# load devotion_lexicon
user_dict <- scan(file = "./dict/user_dict.txt", what=character(),sep='\n',
encoding='utf-8',fileEncoding='utf-8')
stop_words_df <- fread(file = "./dict/stop_words.txt", sep='\n'
,encoding='UTF-8', colClasses="character")
stop_words <- stop_words_df %>% pull(1)
negation_words <- scan(file = "./dict/negation_words.txt", what=character(),sep='\n')
# 將bigram拆成word1和word2
# 將包含英文字母或和數字的詞彙清除
bigrams_separated <- article_comment_bigram %>%
filter(!str_detect(bigram, regex("[0-9a-zA-Z]"))) %>%
separate(bigram, c("word1", "word2"), sep = " ")
# 並選出word2爲情緒詞的bigram
#去除wrod1與word2都是stop word
article_comment_sentiment_bigrams <- bigrams_separated %>%
filter(!(word1 %in% stop_words & word2 %in% stop_words)) %>%
filter(word2 %in% LIWC_ch$word)
article_comment_sentiment_bigrams
# 選出word2中，有出現在情緒詞典中的詞彙
# 如果是正面詞彙則賦予： 情緒標籤爲"positive"、情緒值爲  1
# 如果是負面詞彙則賦予： 情緒標籤爲"negative"、情緒值爲 -1
#將正負面詞分開
LIWC_ch_pos <- LIWC_ch %>% filter(sentiment == "positive")
LIWC_ch_neg <- LIWC_ch %>% filter(sentiment == "negative")
article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>%
select(source, artDate, word1, word2) %>%
mutate(sentiment = ifelse(word2 %in% LIWC_ch_pos$word,1,-1), sentiment_tag = ifelse(word2 %in% LIWC_ch_pos$word, "positive", "negative"))
article_comment_sentiment_bigrams
# 生成一個時間段中的 日期和情緒標籤的所有可能組合
all_dates <-
expand.grid(seq(as.Date(min(article_comment_sentiment_bigrams$artDate)), as.Date(max(article_comment_sentiment_bigrams$artDate)), by="day"), c("positive", "negative"))
names(all_dates) <- c("artDate", "sentiment")
all_dates
# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- all_dates %>%
merge(sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
group_by(source, artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- rbind(
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "nsysu_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nsysu_dcard"),
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "nuk_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nuk_dcard"),
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "exam_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "exam_dcard"),
all_dates %>%
merge(sentiment_plot_data %>%
filter(source == "ptt")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "ptt")
) %>%
mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d")) +
facet_wrap(~source)
# 查看 前面出現否定詞 且 後面爲情緒詞彙 的組合
article_comment_sentiment_bigrams %>%
filter(word1 %in% negation_words) %>%
count(word1, word2, sort = TRUE)
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- all_dates %>%
merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 最後把圖畫出來
negated_sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(source, artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- rbind(
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "nsysu_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nsysu_dcard"),
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "nuk_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "nuk_dcard"),
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "exam_dcard")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "exam_dcard"),
all_dates %>%
merge(negated_sentiment_plot_data %>%
filter(source == "ptt")
,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(source = "ptt")
) %>%
mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data
# 畫圖
sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment))+
scale_x_date(labels = date_format("%m/%d")) +
facet_wrap(~source)
# ngram function, where n=11
ngram_11 <- function(t) {
lapply(t, function(x) {
if(nchar(x)>1){
tokens <- segment(x, jieba_tokenizer)
ngram<- ngrams(tokens, 11)
ngram <- lapply(ngram, paste, collapse = " ")
unlist(ngram)
}
})
}
# 執行ngram_11進行分詞
article_comment_ngram_11 <- article_comment %>%
select(id, artContent) %>%
unnest_tokens(ngram, artContent, token = ngram_11) %>%
filter(!str_detect(ngram, regex("[0-9a-zA-Z]")))
article_comment_ngram_11
# 將ngram拆成word1 ~ word11
ngrams_11_separated <- article_comment_ngram_11 %>%
separate(ngram, paste0("word", c(1:11),sep=""), sep = " ")
ngrams_11_separated
# 尋找 "中山" 出現的前後五個詞彙
tu_five_words <- ngrams_11_separated %>%
filter((word6=="中山"))
tu_five_words
# 尋找 "中山" 的前後5個詞中常出現哪些的詞彙
tu_five_words_count <- tu_five_words %>%
melt(id.vars = "id", measure.vars = paste0("word", c(1:11),sep="")) %>%
rename(word=value) %>%
filter(variable!="word6") %>%
filter(!(word %in% stop_words), nchar(word)>1) %>%
count(word, sort = TRUE)
tu_five_words_count
# 畫圖顯示
tu_five_words_count %>%
arrange(desc(abs(n))) %>%
head(20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = n > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words near by \"中山") +
ylab("Word count") +
coord_flip()+
theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
# 計算兩個詞彙同時出現的總次數
id_total_tokens <- total_tokens %>%
count(id, word, sort = TRUE)
word_pairs <- id_total_tokens %>%
pairwise_count(word, id, sort = TRUE)
word_pairs
# 計算兩個詞彙間的相關性
word_cors <- id_total_tokens %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, id, sort = TRUE)
word_cors
# 顯示相關性大於0.4的組合
set.seed(2019)
word_cors %>%
filter(correlation > .4) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), repel = TRUE, family = "Heiti TC Light") + #加入中文字型設定，避免中文字顯示錯誤。
theme_void()
View(article_comment)
View(article_comment_sentiment_bigrams)
View(word_cors)
View(word_pairs)
View(article_comment_ngram_11)
View(article_comment_sentiment_bigrams)
View(article_comment_sentiment_bigrams_negated)
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
# 查看 前面出現否定詞 且 後面爲情緒詞彙 的組合
article_comment_sentiment_bigrams %>%
filter(word1 %in% negation_words) %>%
count(word1, word2, sort = TRUE)
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
group_by(artDate,sentiment_tag) %>%
summarise(count=n())
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- all_dates %>%
merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
all.x=T,all.y=T) %>%
mutate(count = replace_na(count, 0))
# 最後把圖畫出來
negated_sentiment_plot_data %>%
ggplot()+
geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
scale_x_date(labels = date_format("%m/%d"))
View(article_comment_sentiment_bigrams_negated)
negative <- strsplit(n, "[,]")[[1]]
p <- read_file("dict/nsysu_positive.txt")
n <- read_file("dict/nsysu_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nsysu_ch <- rbind(positive, negative)
nsysu_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nsysu_ch)
student_dcard %>%
inner_join(nsysu_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nuk_ch)
student_dcard %>%
inner_join(nuk_ch)
p <- read_file("dict/nuk_positive.txt")
n <- read_file("dict/nuk_negative.txt")
positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nuk_ch <- rbind(positive, negative)
nuk_ch
sentiment_dcard = student_dcard %>%
filter(nchar(.$word)>1) %>%
group_by(source,word) %>%
summarise(sum = n()) %>%
filter(sum>1) %>%
arrange(desc(sum))%>%
inner_join(nuk_ch)
student_dcard %>%
inner_join(nuk_ch)
plot_table<-sentiment_dcard %>%
group_by(source,sentiment) %>%
summarise(count=sum(sum))
plot_table %>%
ggplot(aes( sentiment,count,fill=sentiment))+
geom_bar(stat="identity", width=0.5)
