---
title: "SMA"
author: "Bolun Lin"
date: "2019年4月19日   "
output: 
  html_document:
    highlight: pygments
    theme: flatly
    css: style.css
---

### 準備所需要的library
```{r message=FALSE, warning=FALSE}
packages = c("dplyr", "tidytext", "stringr", "wordcloud2","tidyverse",'knitr','kableExtra','NLP', "ggplot2",'readr','data.table','reshape2','wordcloud','tidyr','scales','jiebaR','sentimentr','htmltools',"ggraph", "igraph", "reshape2", "widyr","ggplot")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
```
### 載入 library
```{r message=FALSE, warning=FALSE} 

library(dplyr)
library(stringr)
require(tidytext)
library(sentimentr)
library(wordcloud2)
require(data.table)
require(ggplot2)
require(reshape2)
require(wordcloud)
require(tidyr)
require(readr)
require(scales)
library(jiebaR)
library(tidyverse)
library(knitr)
library(kableExtra)
library(NLP)
library(ggraph)
library(plotly)
library(igraph)
library(reshape2)
library(widyr)
```

### 載入文章及留言
```{r}
nsysu_data = fread("data/my_csv_nsysu.csv",encoding = 'UTF-8')
nuk_data =fread("data/my_csv_nuk.csv",encoding = 'UTF-8')
exam_data =fread("data/my_csv_exam.csv",encoding = 'UTF-8')
ptt_data =fread("data/ptt.csv",encoding = 'UTF-8')
nsysu_comment =fread("data/comment_nsysu.csv",encoding = 'UTF-8') #552
nuk_comment =fread("data/comment_nuk.csv",encoding = 'UTF-8') #818
ptt_comment =fread("data/comment_ptt.csv",encoding = 'UTF-8') #1024
exam_comment =fread("data/comment_exam.csv",encoding = 'UTF-8') #1024
# apple_data =fread("data/merge_apple_articleMetaData.csv",encoding = 'UTF-8') #蘋果資料不佳
```

### 整理文章資料
```{r}
#過濾有出現關於併校資訊關鍵字的文章，並且將增加來源的欄位
nsysu_data = nsysu_data %>%
  filter(grepl("高大|併校|合併|合校|高雄大學|公聽會",content)|grepl("高大|併校|合併|合校|高雄大學|公聽會",title))%>%
  mutate(source = "nsysu_dcard")

nuk_data = nuk_data %>%
   filter(grepl("中山|併校|合併|合校|中山大學|公聽會",content)|grepl("中山|併校|合併|合校|中山大學|公聽會",title)) %>%
   filter(!(grepl("愛滋",content)|grepl("愛滋",title))) %>%
   mutate(source = "nuk_dcard")

exam_data = exam_data %>%
   filter(grepl("併校|合併|合校|公聽會",content)|grepl("併校|合併|合校|公聽會",title)) %>%
   filter(grepl("高大|中山|高雄大學",content)|grepl("高大|中山|高雄大學",title)) %>%
  mutate(source = "exam_dcard")

ptt_data = ptt_data %>%
   filter(grepl("併校|合併|合校|公聽會",artTitle)|grepl("併校|合併|合校|公聽會",artContent)) %>%
   filter(grepl("高大|中山|高雄大學",artTitle)|grepl("高大|中山|高雄大學",artContent)) %>%
   filter(!(grepl("山手線|賽馬|手線|高雄縣市合併|又老又窮|清大或交大合校",artTitle)|grepl("山手線|賽馬|手線|大甲",artContent))) %>% 
  mutate(source = "ptt")
#將ptt的資料_id 改成統一的id
colnames(ptt_data)[which(names(ptt_data) == "_id")] <- "id"


head(nuk_data,10)
head(nuk_data,10)
head(exam_data,10)
head(ptt_data,10)
```
 總共有 30篇dcard中山板的文、30篇高大板的文章、29篇ptt板的文章
### 將文章資料合在一起
```{r}
#透過rbind將文章資料合起來
article = nsysu_data %>%
   select(id,createdAt ,source ,title,content)%>% 
   rbind(nuk_data %>% select(id,createdAt ,source ,title,content))%>%
   rbind(exam_data %>% select(id,createdAt ,source ,title,content))
colname = c("id", "artDate","source","artTitle","artContent")
colnames(article) <- colname
article = article %>%
  rbind(ptt_data %>% select(id,artDate ,source ,artTitle,artContent))

article$artDate = as.Date(article$artDate)
article
```
### 整理留言資料
```{r}
nsysu_comment = nsysu_comment %>%
  filter(nchar(nsysu_comment$content)>5)%>% #412
  mutate(source="nsysu_dcard")
 nuk_comment = nuk_comment %>%
  filter(nchar(nuk_comment$content)>5)%>% #501
   mutate(source="nuk_dcard")
 exam_comment = exam_comment %>%
  filter(nchar(exam_comment$content)>5)%>% #978
  mutate(source="exam_dcard")
ptt_comment = ptt_comment %>%
  filter(nchar(ptt_comment$content)>5)%>% #386
  mutate(source="ptt")
head(nsysu_comment,10)
head(nuk_comment,10)
head(exam_comment,10)
head(ptt_comment,10)
```
### 將留言資料合在一起
```{r}
comment = nsysu_comment %>%
   select(id,Date ,source ,source,content)%>% 
   rbind(nuk_comment %>% select(id,Date ,source ,content))%>%
  rbind(exam_comment %>% select(id,Date ,source ,content))

comment =comment %>%
  rbind(ptt_comment %>% select(id,Date ,source ,source,content))
colname = c("id", "artDate","source","artContent")
colnames(comment) <- colname
comment$artDate = as.Date(comment$artDate)
comment <- comment %>% 
  filter(id %in% article$id)

comment
```
### 再將留言資料和文章資料合起來，再一日期計算文章數
```{r}
data = article%>%
  smartbind(comment) %>% 
  select(artDate, id,source) %>% 
  distinct()

data_count_by_date <- data %>% 
  group_by(artDate,source) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

head(data_count_by_date, 20)

```

```{r message=FALSE, warning=FALSE}
plot_date <- 
  data_count_by_date %>% 
  ggplot(aes(x = as.Date(artDate), y = count,colour=source)) +
  geom_line(size = 0.5) + 
  # geom_vline(xintercept = as.numeric(as.Date("2019-03-30")), col='red') + 
  scale_x_date(labels = date_format("%Y/%m/%d" )) +
  ggtitle("高大與中山併校 討論文章數") + 
  xlab("日期") + 
  ylab("數量") + 
  theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。

ggplotly(plot_date)
```



```{r}
jieba_tokenizer <- worker(user="dict/user_dict.txt", stop_word = "dict/stop_words.txt")
clean = function(txt) {
  txt = gsub("B\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
  txt = gsub("(http|https)://.*", "", txt) #去除網址
  txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
  txt = gsub("\\n"," ",txt) #去除換行
  txt = gsub("\\s+"," ",txt) #去除一個或多個空格
  txt = gsub("^\\s+|\\s+$","",txt) #去除前後一個或多個空格
  txt = gsub("&.*;","",txt) #去除html特殊字元編碼
  txt = gsub("[a-zA-Z0-9?!. ']","",txt) #除了字母,數字 ?!. ,空白的都去掉
  txt }

tokenizer <- function(t) {
  lapply(t, function(x) {
    tokens <- segment(x, jieba_tokenizer)
    return(tokens)
  })
}
article_tokens <- article %>% 
  unnest_tokens(word, artContent, token=tokenizer)
article_tokens$word = clean(article_tokens$word)
article_tokens = article_tokens %>%
  filter(!word == "")

comment_tokens <- comment %>% 
  unnest_tokens(word, artContent, token=tokenizer)
comment_tokens$word = clean(comment_tokens$word)
comment_tokens = comment_tokens %>%
  filter(!word == "")
```
# 文字雲
```{r}
# 計算詞彙的出現次數，如果詞彙只有一個字則不列入計算
article_tokens_count <- article_tokens %>% 
  filter(nchar(.$word)>1) %>%
  group_by(word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum))
comment_tokens_count <- comment_tokens %>% 
  filter(nchar(.$word)>1) %>%
  group_by(word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum))
# 印出最常見的20個詞彙
head(article_tokens_count, 20)
head(comment_tokens_count, 20)

```
```{r}

article_tokens_count %>% wordcloud2()

comment_tokens_count %>% wordcloud2()

article_tokens_count %>%
  filter(!word %in% c("中山","大學","中山大學","高大","學校","高雄大學","國立","合校"))%>%
  mutate(word = reorder(word, sum)) %>%
  top_n(25,sum) %>%
  ggplot(aes(word, sum)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
comment_tokens_count %>%
  filter(!word %in% c("中山","大學","中山大學","高大","學校","高雄大學","國立","合校"))%>%
  mutate(word = reorder(word, sum)) %>%
  top_n(25,sum) %>%
  ggplot(aes(word, sum)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()


```

探討dcar上高大同學以及中山同學對於併校的看法
```{r}
library(gtools)
student_dcard =article_tokens %>%
  smartbind(comment_tokens)
student_dcard = student_dcard%>%
  filter(source %in% c("nsysu_dcard","nuk_dcard"))
  
student_dcard%>%
  group_by(source) %>% 
  summarise(sum = n()) %>%
  ggplot(aes(source,
             sum))+ 
  geom_bar(stat="identity", width=0.5,fill="steelblue")+
  geom_text(aes(label=sum), vjust=-0.3, size=3.5)
```
```{r}
frequency <- student_dcard%>%
  dplyr::count(source, word)%>%
  group_by(source) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(source, proportion) %>% 
  gather(source, proportion, `nuk_dcard`)
frequency

```

```{r}
ggplot(frequency, aes(x = proportion, y = `nsysu_dcard`, color = abs(`nsysu_dcard` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.2, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75")+
  theme(legend.position="none") +
  labs(y = "nsysu_dcard", x = "nuk_dcard")
```

```{r}
cor.test(data = frequency[frequency$source == "nuk_dcard",],
         ~ proportion + `nsysu_dcard`)
```
#準備LIWC字典 
```{r}
p <- read_file("dict/positive.txt")
n <- read_file("dict/negative.txt")

positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
LIWC_ch <- rbind(positive, negative)
LIWC_ch
```


## 情緒字典
```{r}
sentiment_dcard = student_dcard %>%
   filter(nchar(.$word)>1) %>%
  group_by(source,word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum))%>%
  inner_join(LIWC_ch)
student_dcard %>%
  inner_join(LIWC_ch)


 student_dcard %>%
   filter(nchar(.$word)>1) %>%
  group_by(source,word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum)) %>%
   filter(source =="nsysu_dcard")
 student_dcard %>%
   filter(nchar(.$word)>1) %>%
  group_by(source,word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum)) %>%
   filter(source =="nuk_dcard")

```


```{r}
plot_table<-sentiment_dcard %>%
  group_by(source,sentiment) %>%
  summarise(count=sum(sum)) 

# interaction(source, sentiment)
plot_table %>%
  ggplot(aes( sentiment,count,fill=sentiment))+
  geom_bar(stat="identity", width=0.5)+
  facet_grid(~source)
```

```{r}

```











看不出差異，手動增加字典





```{r}
p <- read_file("dict/nsysu_positive.txt")
n <- read_file("dict/nsysu_negative.txt")

positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nsysu_ch <- rbind(positive, negative)
nsysu_ch
```

```{r}
sentiment_dcard = student_dcard %>%
   filter(nchar(.$word)>1) %>%
  group_by(source,word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum))%>%
  filter(source =="nsysu_dcard")%>%
  inner_join(nsysu_ch)
student_dcard %>%
  filter(source =="nsysu_dcard")%>%
  inner_join(nsysu_ch)
```


```{r}
plot_table<-sentiment_dcard %>%
  group_by(source,sentiment) %>%
  summarise(count=sum(sum)) 
plot_table %>%
  ggplot(aes( sentiment,count,fill=sentiment))+
  geom_bar(stat="identity", width=0.5)
```

```{r}
p <- read_file("dict/nuk_positive.txt")
n <- read_file("dict/nuk_negative.txt")

positive <- strsplit(p, "[,]")[[1]]
negative <- strsplit(n, "[,]")[[1]]
positive <- data.frame(word = positive, sentiments = "positive")
negative <- data.frame(word = negative, sentiemtns = "negative")
colnames(negative) = c("word","sentiment")
colnames(positive) = c("word","sentiment")
nuk_ch <- rbind(positive, negative)
nuk_ch
```
```{r}
sentiment_dcard = student_dcard %>%
   filter(nchar(.$word)>1) %>%
  group_by(source,word) %>% 
  summarise(sum = n()) %>% 
  filter(sum>1) %>%
  arrange(desc(sum))%>%
  filter(source=="nuk_dcard")%>%
  inner_join(nuk_ch)
student_dcard %>%
  filter(source=="nuk_dcard")%>%
  inner_join(nuk_ch)
```

```{r}
plot_table<-sentiment_dcard %>%
  group_by(source,sentiment) %>%
  summarise(count=sum(sum)) 
plot_table %>%
  ggplot(aes( sentiment,count,fill=sentiment))+
  geom_bar(stat="identity", width=0.5)
```

#計算tf-idf找出各種板討論中較獨特的詞彙
```{r}
#合併文章與留言的tokens
total_tokens <- article_tokens %>% 
  smartbind(comment_tokens)

#計算各板的總詞彙數
total_tokens_count <- total_tokens %>% 
  count(source, word)
total_words_counts <- total_tokens_count %>% 
  group_by(source) %>% 
  summarise(total = sum(n))

total_words <- left_join(total_tokens_count, total_words_counts)

#bind tf-idf
total_words_tf_idf <- total_words %>%
  bind_tf_idf(word, source, n)
total_words_tf_idf
```

```{r}
total_words_tf_idf %>% 
  group_by(source) %>% 
  top_n(10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = source)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~source, ncol = 2, scales = "free") +
  coord_flip()
```

#各板詞彙比例與詞頻長條圖
```{r}
ggplot(total_words, aes(n/total, fill = source)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.01) +
  facet_wrap(~source, ncol = 2, scales = "free_y")
```

#討論併校的文章是否符合zip'f law
```{r}
freq_by_rank <- total_words %>% 
  group_by(source) %>% 
  arrange(desc(n)) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = source)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```
```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = source)) + 
  geom_abline(intercept = -1.417  , slope = -0.730, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

#ngram

###bigram function
```{r}
# remove stopwords

# unnest_tokens 使用的bigram分詞函數
# Input: a character vector
# Output: a list of character vectors of the same length
jieba_bigram <- function(t) {
  lapply(t, function(x) {
    if(nchar(x)>1){
      tokens <- segment(x, jieba_tokenizer)
      bigram<- ngrams(tokens, 2)
      bigram <- lapply(bigram, paste, collapse = " ")
      unlist(bigram)
    }
  })
}
```

##執行bigram斷詞
```{r}
article_comment <- article %>% 
  smartbind(comment)
article_comment_bigram <- article_comment %>%
  unnest_tokens(bigram, artContent, token = jieba_bigram)

article_comment_bigram
```

##載入各種字典
```{r}
# load devotion_lexicon
user_dict <- scan(file = "./dict/user_dict.txt", what=character(),sep='\n', 
                   encoding='utf-8',fileEncoding='utf-8')
stop_words_df <- fread(file = "./dict/stop_words.txt", sep='\n'
                   ,encoding='UTF-8', colClasses="character")

stop_words <- stop_words_df %>% pull(1)

negation_words <- scan(file = "./dict/negation_words.txt", what=character(),sep='\n')


```

##ngram 結合 情緒分析
```{r}
# 將bigram拆成word1和word2
# 將包含英文字母或和數字的詞彙清除
bigrams_separated <- article_comment_bigram %>%
  filter(!str_detect(bigram, regex("[0-9a-zA-Z]"))) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# 並選出word2爲情緒詞的bigram
#去除wrod1與word2都是stop word
bigrams_separated  <- bigrams_separated %>%
  filter(!(word1 %in% stop_words & word2 %in% stop_words)) 

article_comment_sentiment_bigrams <- rbind(
  bigrams_separated %>% 
    filter(source == "nsysu_dcard") %>% 
    merge(nsysu_ch , by.x='word2', by.y='word')
  ,
  bigrams_separated %>% 
    filter(source == "nuk_dcard") %>% 
    merge(nuk_ch , by.x='word2', by.y='word')
  ,
  bigrams_separated %>% 
    filter(source == "ptt" | source == "exam_dcard") %>% 
    merge(LIWC_ch , by.x='word2', by.y='word')
)    

article_comment_sentiment_bigrams 
```





```{r}
# 選出word2中，有出現在情緒詞典中的詞彙
# 如果是正面詞彙則賦予： 情緒標籤爲"positive"、情緒值爲  1
# 如果是負面詞彙則賦予： 情緒標籤爲"negative"、情緒值爲 -1

#將正負面詞分開

article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>% rename(sentiment_tag = sentiment)

article_comment_sentiment_bigrams <- article_comment_sentiment_bigrams %>% 
  mutate(sentiment = ifelse(sentiment_tag == "positive",1,-1)) %>%
  select(source, artDate, word1, word2, sentiment_tag, sentiment)
  

article_comment_sentiment_bigrams
```
```{r}
article_comment_sentiment_bigrams %>%
  filter(source =='nuk_dcard') %>%
  count(sentiment_tag) %>% View()
```

```{r}
# 生成一個時間段中的 日期和情緒標籤的所有可能組合
all_dates <- 
  expand.grid(seq(as.Date(min(article_comment_sentiment_bigrams$artDate)), as.Date(max(article_comment_sentiment_bigrams$artDate)), by="day"), c("positive", "negative"))
names(all_dates) <- c("artDate", "sentiment")
all_dates

# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
  group_by(artDate,sentiment_tag) %>%
  summarise(count=n())  
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- all_dates %>% 
  merge(sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data

# 畫圖
sentiment_plot_data %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
  scale_x_date(labels = date_format("%m/%d")) 
```

##各版的情緒走勢圖

```{r}

# 計算我們資料集中 每日的情緒值
sentiment_plot_data <- article_comment_sentiment_bigrams %>%
  group_by(source, artDate,sentiment_tag) %>%
  summarise(count=n())  

# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
sentiment_plot_data <- rbind(
  all_dates %>% 
  merge(sentiment_plot_data %>% 
          filter(source == "nsysu_dcard")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "nsysu_dcard"), 
  all_dates %>% 
  merge(sentiment_plot_data %>% 
          filter(source == "nuk_dcard")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "nuk_dcard"), 
  all_dates %>% 
  merge(sentiment_plot_data %>% 
          filter(source == "exam_dcard")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "exam_dcard"), 
  all_dates %>% 
  merge(sentiment_plot_data %>% 
          filter(source == "ptt")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "ptt") 
  ) %>% 
  mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data

# 畫圖
sentiment_plot_data %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment))+
  scale_x_date(labels = date_format("%m/%d")) + 
  facet_wrap(~source)
```


##使用否定詞改變詞彙情緒值
```{r}
# 查看 前面出現否定詞 且 後面爲情緒詞彙 的組合
article_comment_sentiment_bigrams %>%
  filter(word1 %in% negation_words) %>%
  count(word1, word2, sort = TRUE) 
```

```{r}
# 如果在情緒詞前出現的是否定詞的話，則將他的情緒對調
article_comment_sentiment_bigrams_negated <- article_comment_sentiment_bigrams %>%
  mutate(sentiment=ifelse(word1 %in% negation_words, -1*sentiment, sentiment)) %>%
  mutate(sentiment_tag=ifelse(sentiment>0, "positive", "negative"))
article_comment_sentiment_bigrams_negated
```

#畫情緒走勢圖
```{r}
# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
  group_by(artDate,sentiment_tag) %>%
  summarise(count=n())  
# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- all_dates %>% 
  merge(negated_sentiment_plot_data,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(count = replace_na(count, 0))
# 最後把圖畫出來
negated_sentiment_plot_data %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment), size = 1.2)+
  scale_x_date(labels = date_format("%m/%d")) 
```

##各版的情緒走勢圖

```{r}

# 計算我們資料集中 每日的情緒值
negated_sentiment_plot_data <- article_comment_sentiment_bigrams_negated %>%
  group_by(source, artDate,sentiment_tag) %>%
  summarise(count=n())  

# 將所有 "日期與情緒值的所有可能組合" 與 "每日的情緒值" join起來
# 如果資料集中某些日期沒有文章或情緒值，會出現NA
# 我們用0取代NA
negated_sentiment_plot_data <- rbind(
  all_dates %>% 
  merge(negated_sentiment_plot_data %>% 
          filter(source == "nsysu_dcard")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "nsysu_dcard"), 
  all_dates %>% 
  merge(negated_sentiment_plot_data %>% 
          filter(source == "nuk_dcard")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "nuk_dcard"), 
  all_dates %>% 
  merge(negated_sentiment_plot_data %>% 
          filter(source == "exam_dcard")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "exam_dcard"), 
  all_dates %>% 
  merge(negated_sentiment_plot_data %>% 
          filter(source == "ptt")
        ,by.x=c('artDate', "sentiment"),by.y=c('artDate', "sentiment_tag"),
        all.x=T,all.y=T) %>% 
  mutate(source = "ptt") 
  ) %>% 
  mutate(count = replace_na(count, 0))
# 時間區段中，每日的情緒值
sentiment_plot_data

# 畫圖
sentiment_plot_data %>%
  ggplot()+
  geom_line(aes(x=artDate,y=count,colour=sentiment))+
  scale_x_date(labels = date_format("%m/%d")) + 
  facet_wrap(~source)
```

#11gram - 尋找特定詞彙的前後5個詞彙
```{r}
# ngram function, where n=11
ngram_11 <- function(t) {
  lapply(t, function(x) {
    if(nchar(x)>1){
      tokens <- segment(x, jieba_tokenizer)
      ngram<- ngrams(tokens, 11)
      ngram <- lapply(ngram, paste, collapse = " ")
      unlist(ngram)
    }
  })
}
```

```{r}
# 執行ngram_11進行分詞
article_comment_ngram_11 <- article_comment %>%
  select(id, artContent) %>%
  unnest_tokens(ngram, artContent, token = ngram_11) %>%
  filter(!str_detect(ngram, regex("[0-9a-zA-Z]")))
article_comment_ngram_11
```
```{r}
# 將ngram拆成word1 ~ word11
ngrams_11_separated <- article_comment_ngram_11 %>%
  separate(ngram, paste0("word", c(1:11),sep=""), sep = " ")
ngrams_11_separated
```

```{r}
# 尋找 "中山" 出現的前後五個詞彙
tu_five_words <- ngrams_11_separated %>%
  filter((word6=="中山"))
tu_five_words
```


```{r}
# 尋找 "中山" 的前後5個詞中常出現哪些的詞彙
tu_five_words_count <- tu_five_words %>%
  melt(id.vars = "id", measure.vars = paste0("word", c(1:11),sep="")) %>%
  rename(word=value) %>%
  filter(variable!="word6") %>%
  filter(!(word %in% stop_words), nchar(word)>1) %>%
  count(word, sort = TRUE)
tu_five_words_count
```

```{r}
# 畫圖顯示
tu_five_words_count %>%
  arrange(desc(abs(n))) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = n > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words near by \"中山") +
  ylab("Word count") +
  coord_flip()+ 
  theme(text = element_text(family = "Heiti TC Light")) #加入中文字型設定，避免中文字顯示錯誤。
```

##Word Correlation
```{r}
# 計算兩個詞彙同時出現的總次數
id_total_tokens <- total_tokens %>%
  count(id, word, sort = TRUE) 

word_pairs <- id_total_tokens %>%
  pairwise_count(word, id, sort = TRUE)
word_pairs
```

```{r}
# 計算兩個詞彙間的相關性
word_cors <- id_total_tokens %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, id, sort = TRUE)
word_cors
```
```{r}
# 顯示相關性大於0.4的組合
set.seed(2019)
word_cors %>%
  filter(correlation > .4) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), repel = TRUE, family = "Heiti TC Light") + #加入中文字型設定，避免中文字顯示錯誤。
  theme_void()
```

